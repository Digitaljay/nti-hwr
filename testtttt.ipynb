{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"testtttt.ipynb","provenance":[],"machine_shape":"hm","mount_file_id":"1eq3JtHOAVSyMzGlG4Rn36NEzp6v7knK7","authorship_tag":"ABX9TyOdX5zvubMSoyNLOMd21AoT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_pjFs4wb-rN","executionInfo":{"status":"ok","timestamp":1646380123861,"user_tz":-300,"elapsed":3055,"user":{"displayName":"Елизавета Пушкарева","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12230354664042372249"}},"outputId":"4c05c0b7-de92-4068-d8c4-d1db88517abc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.6.0+cu101)\n","Requirement already satisfied: torchvision==0.7.0+cu101 in /usr/local/lib/python3.7/dist-packages (0.7.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (1.21.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0+cu101) (7.1.2)\n"]}],"source":["!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","source":["!python -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yzOqu505cEGX","executionInfo":{"status":"ok","timestamp":1646380127364,"user_tz":-300,"elapsed":3512,"user":{"displayName":"Елизавета Пушкарева","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12230354664042372249"}},"outputId":"00bf91be-3dd4-40bc-cfc0-4bd5fe70aa82"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html\n","Requirement already satisfied: detectron2 in /usr/local/lib/python3.7/dist-packages (0.4+cu101)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.63.0)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n","Requirement already satisfied: iopath>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.9)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n","Requirement already satisfied: omegaconf>=2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.1.1)\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n","Requirement already satisfied: fvcore<0.1.4,>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.3.post20210317)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.1.1)\n","Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.4,>=0.1.3->detectron2) (1.21.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.4,>=0.1.3->detectron2) (6.0)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath>=0.1.2->detectron2) (2.4.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from omegaconf>=2->detectron2) (4.8)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2) (1.15.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.44.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.6)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.17.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.37.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.35.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.11.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->detectron2) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->detectron2) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.2.0)\n"]}]},{"cell_type":"code","source":["!pip install tensorflow==2.1.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9CSsF1PscGdV","executionInfo":{"status":"ok","timestamp":1646380130614,"user_tz":-300,"elapsed":3276,"user":{"displayName":"Елизавета Пушкарева","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12230354664042372249"}},"outputId":"2e402e82-cc0b-4bd6-9dfd-ae02ca0f3a8a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.7/dist-packages (2.1.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.0.8)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.21.5)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.13.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.15.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.44.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.0)\n","Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (2.1.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (3.17.3)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.2.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.37.1)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.4.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.1.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (1.0.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (0.8.1)\n","Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.1.0) (2.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (3.1.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.6)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.35.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.11.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.1.0) (1.5.2)\n"]}]},{"cell_type":"code","source":["!pip install opencv-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2j_h4V2cKYw","executionInfo":{"status":"ok","timestamp":1646380133980,"user_tz":-300,"elapsed":3379,"user":{"displayName":"Елизавета Пушкарева","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12230354664042372249"}},"outputId":"f26f3e2d-96b5-4891-91a8-96e93a528a2f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.5)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"9NZKbGvFqqxk","executionInfo":{"status":"ok","timestamp":1646380133982,"user_tz":-300,"elapsed":20,"user":{"displayName":"Елизавета Пушкарева","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12230354664042372249"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6quugQd5cNTl","executionInfo":{"status":"ok","timestamp":1646380137440,"user_tz":-300,"elapsed":3473,"user":{"displayName":"Елизавета Пушкарева","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12230354664042372249"}},"outputId":"5dbb80c4-9c8e-4c55-c9b9-88690a7ff336"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","source":["import json\n","import os\n","import sys\n","import warnings\n","\n","import cv2\n","import numpy as np\n","from PIL import Image, ImageDraw, ImageFont\n","from tqdm import tqdm\n","import pickle\n","\n","warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n","warnings.filterwarnings(\"ignore\")\n","\n","import logging\n","\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","from detectron2.engine import DefaultPredictor\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","logger = logging.getLogger(\"detectron2\")\n","logger.setLevel(logging.CRITICAL)\n","\n","\n","TEST_IMAGES_PATH, SAVE_PATH = \"test_imgs\", \"test_imgs_res.txt\"\n","\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","SEGM_MODEL_PATH = \"segm-model_final.pth\"\n","OCR_MODEL_PATH = \"ocr-model-last.ckpt\"\n","OCR_MODEL_PATH_ENG = \"ocr-model-last_eng.ckpt\"\n","CLF_MODEL_PATH = \"bincl15.pth\"\n","\n","alphabet_eng = ' !\\\"%\\\\\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|}№'\n","\n","CONFIG_JSON = {\n","    \"alphabet\": \" !\\\"#$%&'()*+,-./0123456789:;<=>?@[\\\\]^_`{|}~«»ЁАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№\",\n","    \"image\": {\"width\": 256, \"height\": 32},\n","}\n","CONFIG_JSON_EN = {\n","    \"alphabet\": alphabet_eng,\n","    \"image\": {\"width\": 256, \"height\": 32},\n","}\n","\n","\n","def get_contours_from_mask(mask, min_area=5):\n","    contours, hierarchy = cv2.findContours(\n","        mask.astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n","    )\n","    contour_list = []\n","    for contour in contours:\n","        if cv2.contourArea(contour) >= min_area:\n","            contour_list.append(contour)\n","    return contour_list\n","\n","\n","def get_larger_contour(contours):\n","    larger_area = 0\n","    larger_contour = None\n","    for contour in contours:\n","        area = cv2.contourArea(contour)\n","        if area > larger_area:\n","            larger_contour = contour\n","            larger_area = area\n","    return larger_contour\n","\n","\n","class SEGMpredictor:\n","    def __init__(self, model_path):\n","        cfg = get_cfg()\n","        cfg.merge_from_file(\n","            model_zoo.get_config_file(\n","                \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n","            )\n","        )\n","        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n","            \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n","        )\n","        cfg.MODEL.WEIGHTS = model_path\n","        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n","        cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n","        cfg.INPUT.MIN_SIZE_TEST = 1000\n","        cfg.INPUT.MAX_SIZE_TEST = 1000\n","        cfg.INPUT.FORMAT = \"BGR\"\n","        cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n","\n","        self.predictor = DefaultPredictor(cfg)\n","\n","    def __call__(self, img):\n","        outputs = self.predictor(img)\n","        prediction = outputs[\"instances\"].pred_masks.cpu().numpy()\n","        contours = []\n","        for pred in prediction:\n","            contour_list = get_contours_from_mask(pred)\n","            contours.append(get_larger_contour(contour_list))\n","        return contours\n","\n","\n","OOV_TOKEN = \"<OOV>\"\n","CTC_BLANK = \"<BLANK>\"\n","\n","\n","def get_char_map(alphabet):\n","    \"\"\"Make from string alphabet character2int dict.\n","    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n","    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n","    char_map[CTC_BLANK] = 0\n","    char_map[OOV_TOKEN] = 1\n","    return char_map\n","\n","\n","class Tokenizer:\n","    \"\"\"Class for encoding and decoding string word to sequence of int\n","    (and vice versa) using alphabet.\"\"\"\n","\n","    def __init__(self, alphabet):\n","        self.char_map = get_char_map(alphabet)\n","        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n","\n","    def encode(self, word_list):\n","        \"\"\"Returns a list of encoded words (int).\"\"\"\n","        enc_words = []\n","        for word in word_list:\n","            enc_words.append(\n","                [\n","                    self.char_map[char]\n","                    if char in self.char_map\n","                    else self.char_map[OOV_TOKEN]\n","                    for char in word\n","                ]\n","            )\n","        return enc_words\n","\n","    def get_num_chars(self):\n","        return len(self.char_map)\n","\n","    def decode(self, enc_word_list):\n","        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n","        repeating characters. Also skip out of vocabulary token.\"\"\"\n","        dec_words = []\n","        for word in enc_word_list:\n","            word_chars = \"\"\n","            for idx, char_enc in enumerate(word):\n","                # skip if blank symbol, oov token or repeated characters\n","                if (\n","                    char_enc != self.char_map[OOV_TOKEN]\n","                    and char_enc != self.char_map[CTC_BLANK]\n","                    # idx > 0 to avoid selecting [-1] item\n","                    and not (idx > 0 and char_enc == word[idx - 1])\n","                ):\n","                    word_chars += self.rev_char_map[char_enc]\n","            dec_words.append(word_chars)\n","        return dec_words\n","\n","\n","class to_0:\n","    def __init__(self, height, width):\n","        self.height = height\n","        self.width = width\n","\n","    def __call__(self, x):\n","        x = (x[0] * 0.299 + x[1] * 0.587 + x[2] * 0.114).view(1, self.height, self.width)\n","        return x\n","\n","\n","class Normalize:\n","    def __call__(self, img):\n","        img = img.astype(np.float32) / 255\n","        return img\n","\n","\n","class ToTensor:\n","    def __call__(self, arr):\n","        arr = torch.from_numpy(arr)\n","        return arr\n","\n","\n","class MoveChannels:\n","    \"\"\"Move the channel axis to the zero position as required in pytorch.\"\"\"\n","\n","    def __init__(self, to_channels_first=True):\n","        self.to_channels_first = to_channels_first\n","\n","    def __call__(self, image):\n","        if self.to_channels_first:\n","            return np.moveaxis(image, -1, 0)\n","        else:\n","            return np.moveaxis(image, 0, -1)\n","\n","\n","class ImageResize:\n","    def __init__(self, height, width):\n","        self.height = height\n","        self.width = width\n","\n","    def __call__(self, image):\n","        image = cv2.resize(image, (int(len(image[0])*self.height/len(image)*2), self.height), interpolation=cv2.INTER_LINEAR)\n","        if len(image[0]) <= self.width:\n","            image = np.pad(image, [(0, 0), (0, self.width - len(image[0])), (0, 0)], mode='constant', constant_values=0)\n","        else:\n","            image = cv2.resize(image, (self.width, self.height), interpolation=cv2.INTER_LINEAR)\n","        return image\n","\n","\n","\n","def get_val_transforms(height, width):\n","    transforms = torchvision.transforms.Compose([\n","        ImageResize(height, width),\n","        MoveChannels(to_channels_first=True),\n","        Normalize(),\n","        ToTensor(),\n","        to_0(height, width),\n","    ])\n","    return transforms\n","\n","\n","def get_resnet34_backbone(pretrained=True):\n","    m = torchvision.models.resnet34(pretrained=pretrained)\n","    input_conv = nn.Conv2d(1, 64, 7, 1, 3)\n","    blocks = [input_conv, m.bn1, m.relu, m.maxpool, m.layer1, m.layer2, m.layer3]\n","    return nn.Sequential(*blocks)\n","\n","\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size,\n","            hidden_size,\n","            num_layers,\n","            dropout=dropout,\n","            batch_first=True,\n","            bidirectional=True,\n","        )\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        return out\n","\n","\n","class CRNN(nn.Module):\n","    def __init__(\n","        self,\n","        number_class_symbols,\n","        time_feature_count=256,\n","        lstm_hidden=256,\n","        lstm_len=2,\n","    ):\n","        super().__init__()\n","        self.feature_extractor = get_resnet34_backbone(pretrained=False)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((time_feature_count, time_feature_count))\n","        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(lstm_hidden * 2, time_feature_count),\n","            nn.GELU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(time_feature_count, number_class_symbols),\n","        )\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        b, c, h, w = x.size()\n","        x = x.view(b, c * h, w)\n","        x = self.avg_pool(x)\n","        x = x.transpose(1, 2)\n","        x = self.bilstm(x)\n","        x = self.classifier(x)\n","        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n","        return x\n","\n","\n","def predict(images, model, tokenizer, device):\n","    model.eval()\n","    images = images.to(device)\n","    with torch.no_grad():\n","        output = model(images)\n","    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n","    text_preds = tokenizer.decode(pred)\n","    return text_preds\n","\n","\n","class InferenceTransform:\n","    def __init__(self, height, width):\n","        self.transforms = get_val_transforms(height, width)\n","\n","    def __call__(self, images):\n","        transformed_images = []\n","        for image in images:\n","            image = self.transforms(image)\n","            transformed_images.append(image)\n","        transformed_tensor = torch.stack(transformed_images, 0)\n","        return transformed_tensor\n","\n","\n","class OcrPredictor:\n","    def __init__(self, model_path, config, device=\"cuda\"):\n","        self.tokenizer = Tokenizer(config[\"alphabet\"])\n","        self.device = torch.device(device)\n","        # load model\n","        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n","        self.model.load_state_dict(torch.load(model_path))\n","        self.model.to(self.device)\n","\n","        self.transforms = InferenceTransform(\n","            height=config[\"image\"][\"height\"],\n","            width=config[\"image\"][\"width\"],\n","        )\n","\n","    def __call__(self, images):\n","        if isinstance(images, (list, tuple)):\n","            one_image = False\n","        elif isinstance(images, np.ndarray):\n","            images = [images]\n","            one_image = True\n","        else:\n","            raise Exception(\n","                f\"Input must contain np.ndarray, \"\n","                f\"tuple or list, found {type(images)}.\"\n","            )\n","\n","        images = self.transforms(images)\n","        pred = predict(images, self.model, self.tokenizer, self.device)\n","\n","        if one_image:\n","            return pred[0]\n","        else:\n","            return pred\n","\n","\n","class LangDataset(Dataset):\n","    \"\"\"\n","    Датасет с картинками, который паралельно подгружает их из папок\n","    производит скалирование и превращение в торчевые тензоры\n","    \"\"\"\n","\n","    def __init__(self, files, from_files=True):\n","        super().__init__()\n","        # список файлов для загрузки\n","        self.files = sorted(files)\n","\n","        self.len_ = len(self.files)\n","\n","        self.from_files = from_files\n","\n","    def __len__(self):\n","        return self.len_\n","\n","    def load_sample(self, file):\n","        image = Image.open(file)\n","        image.load()\n","        return image\n","\n","    def load_cv2_sample(self, image):\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        return Image.fromarray(image)\n","\n","    def __getitem__(self, index):\n","        # для преобразования изображений в тензоры PyTorch и нормализации входа\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","        x = self.load_sample(self.files[index]) if self.from_files else self.load_cv2_sample(self.files[index])\n","        x = self._prepare_sample(x)\n","        x = np.array(x / 255, dtype='float32')\n","        x = transform(x)\n","        return x\n","\n","    def _prepare_sample(self, image):\n","        image = image.resize((224, 224))\n","        return np.array(image)\n","\n","def get_image_visualization(img, pred_data, fontpath, font_koef=50):\n","    h, w = img.shape[:2]\n","    font = ImageFont.truetype(fontpath, int(h / font_koef))\n","    empty_img = Image.new(\"RGB\", (w, h), (255, 255, 255))\n","    draw = ImageDraw.Draw(empty_img)\n","\n","    for prediction in pred_data[\"predictions\"]:\n","        polygon = prediction[\"polygon\"]\n","        pred_text = prediction[\"text\"]\n","        cv2.drawContours(img, np.array([polygon]), -1, (0, 255, 0), 2)\n","        x, y, w, h = cv2.boundingRect(np.array([polygon]))\n","        draw.text((x, y), pred_text, fill=0, font=font)\n","\n","    vis_img = np.array(empty_img)\n","    vis = np.concatenate((img, vis_img), axis=1)\n","    return vis\n","\n","\n","def crop_img_by_polygon(img, polygon):\n","    # https://stackoverflow.com/questions/48301186/cropping-concave-polygon-from-image-using-opencv-python\n","    pts = np.array(polygon)\n","    rect = cv2.boundingRect(pts)\n","    x, y, w, h = rect\n","    croped = img[y : y + h, x : x + w].copy()\n","    pts = pts - pts.min(axis=0)\n","    mask = np.zeros(croped.shape[:2], np.uint8)\n","    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n","    dst = cv2.bitwise_and(croped, croped, mask=mask)\n","    return dst\n","\n","\n","class SimpleCnn(nn.Module):\n","    def __init__(self, n_classes):\n","        super().__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3)\n","        )\n","\n","        self.out = nn.Linear(32 * 22 * 22, n_classes)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","\n","        x = x.view(x.size(0), -1)\n","        logits = self.out(x)\n","        return logits\n","\n","def predict_lang(model, test_loader):\n","    with torch.no_grad():\n","        logits = []\n","\n","        for inputs in test_loader:\n","            inputs = inputs.to(DEVICE)\n","            model.eval()\n","            outputs = model(inputs).cpu()\n","            logits.append(outputs)\n","\n","    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n","    return probs\n","\n","\n","\n","class PiepleinePredictor:\n","    def __init__(self, segm_model_path, ocr_model_path_ru, ocr_model_path_en, ocr_config_ru, ocr_config_en, clf_model_path):\n","        self.segm_predictor = SEGMpredictor(model_path=segm_model_path)\n","        self.ocr_predictor_ru = OcrPredictor(model_path=ocr_model_path_ru, config=ocr_config_ru)\n","        self.ocr_predictor_en = OcrPredictor(model_path=ocr_model_path_en, config=ocr_config_en)\n","        self.classification_model = SimpleCnn(2).to(DEVICE)\n","        self.classification_model.load_state_dict(torch.load(clf_model_path))\n","        self.classification_model.eval()\n","        self.labeler = ['eng' 'ru']\n","\n","    def __call__(self, img):\n","        output = {\"predictions\": []}\n","        lang = self.predict_image_lang(img)\n","        contours = self.segm_predictor(img)\n","        if lang == 'ru':\n","            for contour in contours:\n","                if contour is not None:\n","                    crop = crop_img_by_polygon(img, contour)\n","                    pred_text = self.ocr_predictor_ru(crop)\n","                    output[\"predictions\"].append(\n","                        {\n","                            \"polygon\": [[int(i[0][0]), int(i[0][1])] for i in contour],\n","                            \"text\": pred_text,\n","                        }\n","                    )\n","        else:\n","            for contour in contours:\n","                if contour is not None:\n","                    crop = crop_img_by_polygon(img, contour)\n","                    pred_text = self.ocr_predictor_en(crop)\n","                    output[\"predictions\"].append(\n","                        {\n","                            \"polygon\": [[int(i[0][0]), int(i[0][1])] for i in contour],\n","                            \"text\": pred_text,\n","                        }\n","                    )\n","        return output\n","\n","    def predict_image_lang(self, img):\n","        image = DataLoader(\n","            LangDataset([img], False))\n","        return self.labeler[np.argmax(predict_lang(self.classification_model, image))]\n","\n","\n","def main():\n","\n","    pipeline_predictor = PiepleinePredictor(\n","        segm_model_path=SEGM_MODEL_PATH,\n","        ocr_model_path_en=OCR_MODEL_PATH_ENG,\n","        ocr_model_path_ru=OCR_MODEL_PATH,\n","        ocr_config_ru=CONFIG_JSON,\n","        ocr_config_en=CONFIG_JSON_EN,\n","        clf_model_path=CLF_MODEL_PATH\n","    )\n","\n","    pred_data = {}\n","    for img_name in tqdm(os.listdir(TEST_IMAGES_PATH)):\n","        image = cv2.imread(os.path.join(TEST_IMAGES_PATH, img_name))\n","        pred_data[img_name] = pipeline_predictor(image)\n","\n","    with open(SAVE_PATH, \"w\") as f:\n","        json.dump(pred_data, f)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mwge1ZfJeCM1","executionInfo":{"status":"ok","timestamp":1646383157799,"user_tz":-300,"elapsed":10326,"user":{"displayName":"Елизавета Пушкарева","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12230354664042372249"}},"outputId":"01af6206-8505-412f-aad2-6cc371078564"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:05<00:00,  5.76s/it]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"meZ-i5-keFfT","executionInfo":{"status":"aborted","timestamp":1646380145633,"user_tz":-300,"elapsed":35,"user":{"displayName":"Елизавета Пушкарева","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12230354664042372249"}}},"execution_count":null,"outputs":[]}]}